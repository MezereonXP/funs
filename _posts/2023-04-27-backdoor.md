---
title: "浅谈深度学习模型中的后门"
date: 2023-04-27 00:00:00 +0800
math: true
toc: true
categories: [后门攻击]
tags: [后门攻击]
---


关于深度学习安全方面，粗浅地可以分为两大块：**对抗样本（Adversarial Example）**以及**后门（Backdoor）**

关于对抗样本可以查看我之前的文章 ---- [对抗样本攻击](https://mezereonxp.fun/posts/adv-attacks)

这一次我们主要关注深度学习里面的后门攻击。所谓后门，那就是一个隐藏着的，不轻易就被发现的一个通道。在某些特殊情况下，这个通道就会显露出来。

那么在深度学习之中，后门又是怎样的呢？我这里不妨以图像分类任务作为一个例子，我们手里有一张狗的照片，通过分类器，以99%的置信度（confidence）被分类为狗。如若我在这张图像上添加一个图案（比如一个小的红色圆形），通过分类器，以80%的置信度被分类为猫。

那么我们会将这个特殊的图案称之为**触发器（Trigger）**，这个分类器被成为带有后门的分类器。

> 一般来说，后门攻击也就是由这两个部分组成，即触发器以及带有后门的模型

触发器会触发分类器，使其错误分类到指定的类别（当然也可以非指定类别，只是令其出错，一般而言我们谈论的都是指定类别的，如是其他，会特殊说明）。

我们已经将后门攻击介绍了一遍，这边我们主要关注几个问题：

- 如何获得带后门的模型以及对应触发器
- 如何制造隐蔽的后门
- 如何检测模型中的后门

我们这次着重讲第一和第二个问题，如何获得带后门的模型以及对应的触发器。

一般来说，我们会对训练数据进行操作，通过修改训练数据来实现后门攻击，此类手段，称之为**基于投毒**（poisoning-based）的后门攻击。

这里要和**投毒攻击**做出区别，投毒攻击的目的是通过对数据进行投毒，减少模型的泛化能力（Reduce model generalization），而后门攻击的目的是令模型对于带触发器的输入失效，对不带触发器的输入表现正常。


### BadNet

首先我们介绍一下最为经典的攻击，由Gu等人所提出，方式很简单，就是从训练数据集中随机选取样本，添加触发器，并且修改他们的真实标记，然后放回，构建出一个带毒的数据集。
![BadNet](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjA2OTUy,size_16,color_FFFFFF,t_70-20230501014923247.png)

> Gu et al.  Badnets: Evaluating backdooring attacks on deep neural networks

这类方法往往要修改标签，那么有没有不用修改标签的方法呢？

### Clean Label

clean label方法就是不用修改标签，如下图所示，只需要加上一个特殊变换，同时这种方法的触发器比较隐蔽（触发器就是那个对应的变换）。
![Clean label](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjA2OTUy,size_16,color_FFFFFF,t_70-20230501014923403.png)

> Barni et al. A new Backdoor Attack in CNNs by training set corruption without label poisoning

### 更为隐蔽的触发器 Hiding Triggers

Liao等人提出一种生成触发器的方式，该方法会约束触发器对原始图片造成的影响，如下图所示：
![Hiding Triggers](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjA2OTUy,size_16,color_FFFFFF,t_70-20230501014923490.png)
基本上人眼已经无法辨别一张图片是否有触发器了。

> Liao et al. Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation


### 动态的触发器 Dynamic Backdoor

该种方法由Salem等人提出来，通过一个网络动态地决定触发器的位置以及样式，增强了攻击的效果。
![Dynamic Trigger](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjA2OTUy,size_16,color_FFFFFF,t_70-20230501014923575.png)

> Salem et al. Dynamic Backdoor Attacks Against Machine Learning Models