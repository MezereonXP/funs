---
title: "浅谈深度学习算法的公平性"
date: 2023-05-08 14:16:00 +0800
math: true
categories: [AI安全]
tags: [AI公平] 
---

### 介绍

深度学习十分流行，在许多领域有着不错的表现。然而，深度学习算法中的**公平性 (fairness)**亦是重要的研究方向。这次，我们以Du的一篇文章为例，给大家介绍一下深度学习中的公平性。

> **Fairness in Deep Learning: A Computational Perspective**



### 分类

首先我们需要对公平性问题进行分类，然后在分别进行相关工作的介绍。

大致上，公平性问题可以分为：

- **预测输出的歧视 (Prediction Outcome Discrimination)**
    - 对于招聘系统，在评价上会更加偏向于男性群体；对于贷款系统，会对黑人给出较低的信任分数
- **预测质量的差异 (Prediction Quality Disparity)**
    - 比如对于某类群体，模型的任务精度比较低



### 如何度量公平性？

这里介绍几种比较常见的度量指标：

- **人口学平等性 (Demographic Parity)**
- **机会平等性 (Equality of Opportunity)**
- **预测质量平等性 (Predictive Quality Parity)**



**人口学平等性**

该度量指标能够保证算法的决策对于不同群体而言是相似的。比如：
$$
\frac{p(\hat{y}=1|z=0)}{p(\hat{y}=1|z=1)}\geq \tau
$$
其中 $\tau$ 是一个给定的阈值，通常设置成0.8。$z$ 代表需要保护的属性，比如种族、性别等。



**机会平等性**

该度量指标考虑了不同群体当中，标签 $y$ 的分布是有差异的。具体形式为：
$$
p(\hat{y}=1|z=0,y=1)-p(\hat{y}=1|z=1,y=1)
$$
以及
$$
p(\hat{y}=1|z=0,y=0)-p(\hat{y}=1|z=1,y=0)
$$
类似于真阳性 (true positive) 以及假阳性 (false positive) 的概念。



**预测质量平等性**

该度量指标主要是不同群体下的正确率或者精度。



### 与可解释性之间的联系

![可解释性](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402170715183.png)

如图所示，由于人类的造成的偏差，我们的数据集会产生偏差，在神经网络训练的时候，就会放大这个偏差。我们可以通过可解释性来发现数据中可能产生的偏差并且减轻这种偏差。



举个例子，比如贷款系统，如下图所示：

![loan](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402170936870.png)

我们可以通过可解释性，发现拒绝贷款的原因反映在输入的哪一些方面上，进而发现这种歧视。



更为细粒度地，我们可以通过神经元来进行解释，如下图所示：

![global interpretation](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402171219544.png)

通过神经元的激活状态，我们判断其预测的依据，比如低语义的眼球颜色，以及高语义的种族等。



### 对偏差进行检测和修复

![bias detection](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402171556936.png)

如上图所示，将女性医生和男性医生的照片输入网络，判别是否是医生，结果出现了明显的差异。我们通过构建不同的输入集合，可以发现一些决定性的因素，而这些因素，有可能反映了模型中的歧视。



我们可以参照对抗训练 (Adversarial Training) 的方式对模型中的偏差进行修复，比如：

![mitigation](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402171945835.png)

我们通过模型输出，构建一个输出对敏感属性的预测器，反向地回传梯度，使得我们没有办法通过模型输出去预测那些敏感的属性。



同时，我们可以直接在数据的源头上进行公平的标注，然后再在这些数据上进行训练。

![fairness annotation](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402172324726.png)

### 相关数据集

这里介绍两个比较常见的数据集：

- Adult Census Income: 48842个样本，包括年龄、受教育程度、性别、种族等属性以及对应的收入
- COMPAS: 6167个样本，用预测犯罪的一个数据集，也包含许多敏感属性



使用一些去偏差的方法，观察他们的表现，如下表所示：

![discrimination](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210402173223085.png)

对于前两个属性Acc和Parity，代表着正确率以及人口学平等性，越接近1越好。

对于后两个属性Opty和Odds，代表着机会平等性以及投注平等性，越接近0越好。

**可以看到：1. 在提高公平性的时候，正确率通常会下降。2. 不同的公平性度量指标之间可能存在矛盾。**



> [17] Equality of opportunity in supervised learning (NIPS 2016)
>
> [31] Data preprocessing techniques for classification without discrimination (KAIS 2012)
>
> [32] Fairness- aware classifier with prejudice remover regularizer (JEMLKDD 2012)
>
> [33] Optimized pre-processing for discrimination preven- tion (NIPS 2017)



### 研究上的挑战



**Benchmark Dataset**

目前仍然是需要一个比较完备的数据集平台来评估不同的去偏差方法。



**属性交集的公平性 (Intersectional Fairness)**

比如一系列敏感属性的组合的公平性等，目前的工作仍然比较少



**公平性和可用性的取舍**

如何在保持高的可用性下，做到公平性，仍然是个开放性的问题。



**公平性的形式化**

比如公平度量指标的形式化，可能需要一些更加精细的指标来度量深度学习里面的偏差。



**大规模训练下的公平性**

现如今，训练数据的规模越来越大。如何在大规模的训练数据下，保证公平性或者检查出其中的偏差，也是一个开放性的问题。