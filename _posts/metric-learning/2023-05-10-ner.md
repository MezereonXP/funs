---
title: "用于类别增量学习的动态可扩展表征 -- DER"
date: 2023-05-10 15:42:00 +0800
math: true
categories: [度量学习]
tags: [增量学习, 动态] 
---




这次介绍一种类似表征学习的训练方法，用于类别的增量学习，来自于CVPR2021的一篇文章"DER: Dynamically Expandable Representation for Class Incremental Learning"。



首先，我们需要补充一些预先的概念，比如类别增量学习以及表征学习。

### 类别增量学习

传统的分类学习中，我们通常在训练的时候就有全部的类别，测试的时候也是对全部的类别的数据进行测试。

在现实世界中，我们往往不会在一开始就定义完所有的类别，并且收集对应的所有数据，实际情况是，我们通常拥有一部分类别的数据，然后先训练一个分类器，等到有新的类别，再对网络结构等做出调整，重新进行数据收集、训练和测试。



### 表征学习/度量学习

表征学习（Representation Learning），抑或是度量学习（Metric Learning），其目的是，学习到数据的一种表征（通常是一个向量的形式），使得同类的表征距离近，异类的表征距离远，这里的距离可以是欧几里得距离等。

在做类别增量学习的时候，我们往往可以复用先前训练好的表征提取器，在新的数据上进行调优（fine-tune）。



这里，文章将表征学习划分成3类：

- 基于正则化的方法
- 基于蒸馏的方法
- 基于结构的方法



基于正则化的方法一般都会有一个较强的假设，其主要是根据估计的方法，对参数进行微调。

基于蒸馏的方法则是会依赖于所使用的数据的数量和质量。

基于结构的方法，会引入额外的新的参数进来，用来对新类别的数据进行建模。



> 上述这个分类其实不够充分，如果利用传统的度量学习学习一个“前端”，用来抽取特征，然后对后端分类器微调也是一种方法，但这篇文章似乎没有讨论这种方法。



### 基本流程

![pipeline](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210608160901706.png)

如上图所示，其实就是一个特征拼接的过程，首先，我们利用一部分类别的数据进行训练，得到一个特征抽取器 $\Phi_{t-1}$，对于一个新的特征$\mathcal F_t$, 给定一张图片$x\in \tilde{\mathcal D}_t$ , 拼接后的特征可以表示为：

$$
u = \Phi_{t}(x) = [\Phi_{t-1}(x), \mathcal F_t(x)]
$$

然后该特征会输入到一个分类器$\mathcal H_t$上, 输出为:

$$ p_{\mathcal H_t}(y|x) = Softmax(\mathcal H_t(u)) $$

预测结果为:

$$ \hat{y} = \arg\max p_{\mathcal H_t}(y|x) $$

所以，基础的训练误差为简单的交叉熵误差：

$$ \mathcal L_{\mathcal H_t}=-\frac{1}{|\tilde{\mathcal D}_t|}\sum_{i=1}^{|\tilde{\mathcal D}_t|}\log(p_{\mathcal H_t}(y=y_i|x_i)) $$

我们将分类器 $\mathcal H_t$ 替换为对于新类别特征的分类器 $\mathcal H_a$ , 可以得到一个针对新类别特征的误差 $\mathcal L_{\mathcal H_a}$

融合的误差形式为；
$$
\mathcal L_{ER} = \mathcal L_{\mathcal H_t} + \lambda_a\mathcal L_{\mathcal H_a}
$$
为了降低类别增量带来的参数增量，这里引入了一种Mask机制，即学习一个Mask，对通道进行Mask，用一个变量 $e_l$ 进行控制。
$$
f_l'=f_l\odot m_l\\
m_l=\sigma(se_l)
$$
其中 $\sigma(\cdot)$ 表示sigmoid激活函数，$s$ 是一个缩放系数。

引入一个稀疏性误差，用来鼓励模型去尽可能地压缩参数，Mask掉更多的通道：
$$
\mathcal L_S = \frac{\sum_{l=1}^LK_l||m_{l-1}||_1||m_l||_1}{\sum_{l=1}^LK_lc_{l-1}c_{l}}
$$
其中，$L$ 是层的数量，$K_l$ 是第 $l$ 层卷积的Kernel Size。

最终，得到一个综合的误差表达式：
$$
\mathcal L_{DER} = \mathcal L_{\mathcal H_t} +\lambda_a\mathcal L_{\mathcal H_t^a} + \lambda_s\mathcal L_S
$$


### 实验分析



首先是数据集的设置，采用的是三个数据集：

- CIFAR-100
- ImageNet-1000
- Imagenet-100



对于CIFAR-100的100类，会根据5，10，20，50个增量过程来进行训练。这里，对于5个增量过程，也就是每一次会增加20类新的类别数据。这样的数据集分割方法记作CIFAR100-B0。

另外的一种增量方式是，先在50类上进行训练，然后剩下的50类，根据2、5、10个增量过程进行训练。记作CIFAR100-B50。



我们这里仅给出CIFAR-100数据集的结果，更为详细的，可以查看该论文。

![cifar](https://mezereon-upic.oss-cn-shanghai.aliyuncs.com/uPic/image-20210608174051859.png)

如上图所示，该方法最终的平均正确率超过了其他增量学习的方法。需要注意的是，当使用Mask机制是，也就是利用Mask的结果对参数进行裁剪，得到的模型在参数量上降低的很多，正确率仍然能够保持。